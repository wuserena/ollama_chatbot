{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./medquad.csv'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k/viewer/default/train?views%5B%5D=train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek R1 Model Fine-Tuning (LORA) with GPT-4 Dataset [Unsloth and OLLAMA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Unsloth**\n",
    "\n",
    "Unsloth appears to be a tool or framework designed for efficient fine-tuning of language models. From the context, it likely incorporates techniques like Low-Rank Adaptation (LoRA) and other efficiency optimizations to fine-tune large models, such as Llama derivatives, with minimal computational resources.\n",
    "\n",
    "The key features of Unsloth as implied by your description might include:\n",
    "\n",
    "1. **Efficient Fine-Tuning**: Instead of updating the entire model's weights, it leverages methods like LoRA, which fine-tune a smaller subset of the parameters, making it resource-efficient.\n",
    "\n",
    "2. **Simplified Workflow**: The process of loading models, configuring parameters, and training appears streamlined, allowing developers to focus on specific customizations rather than managing complex infrastructure.\n",
    "\n",
    "3. **Integration with Local Runtimes**: After fine-tuning, exporting to tools like Ollama for local deployment demonstrates its support for practical application.\n",
    "\n",
    "## **DeepSeek-R1**\n",
    "\n",
    "The DeepSeek-R1 is a versatile robot for exploration and inspection in tough environments. With AI, precise sensors, and multi-terrain mobility, it handles tasks like data collection, mapping, and monitoring. Customisable for search and rescue, inspections, or research, it ensures reliable performance in hazardous areas.\n",
    "\n",
    "![](https://bsmedia.business-standard.com/_media/bs/img/article/2025-01/28/full/1738047877-0145.JPG?im=FeatureCrop,size=(826,465))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in d:\\ollama\\ollama_env\\lib\\site-packages (2025.3.17)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.3.14 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (2025.3.15)\n",
      "Requirement already satisfied: torch>=2.4.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (2.6.0+cu126)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.0.29.post3)\n",
      "Requirement already satisfied: bitsandbytes in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.45.3)\n",
      "Requirement already satisfied: triton-windows in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (3.2.0.post17)\n",
      "Requirement already satisfied: packaging in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (24.2)\n",
      "Requirement already satisfied: tyro in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.9.17)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (4.49.0)\n",
      "Requirement already satisfied: datasets>=2.16.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (3.3.2)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (7.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: numpy in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (2.2.2)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (1.5.1)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.15.2)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.14.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.28.1)\n",
      "Requirement already satisfied: hf_transfer in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.32.2)\n",
      "Requirement already satisfied: torchvision in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in d:\\ollama\\ollama_env\\lib\\site-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\ollama\\ollama_env\\lib\\site-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\ollama\\ollama_env\\lib\\site-packages (from datasets>=2.16.0->unsloth) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from datasets>=2.16.0->unsloth) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\ollama\\ollama_env\\lib\\site-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\ollama\\ollama_env\\lib\\site-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in d:\\ollama\\ollama_env\\lib\\site-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\ollama\\ollama_env\\lib\\site-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in d:\\ollama\\ollama_env\\lib\\site-packages (from datasets>=2.16.0->unsloth) (3.11.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\ollama\\ollama_env\\lib\\site-packages (from huggingface_hub->unsloth) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\ollama\\ollama_env\\lib\\site-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\ollama\\ollama_env\\lib\\site-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\ollama\\ollama_env\\lib\\site-packages (from torch>=2.4.0->unsloth) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\ollama\\ollama_env\\lib\\site-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\ollama\\ollama_env\\lib\\site-packages (from tqdm->unsloth) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\ollama\\ollama_env\\lib\\site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\ollama\\ollama_env\\lib\\site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\n",
      "Requirement already satisfied: rich in d:\\ollama\\ollama_env\\lib\\site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
      "Requirement already satisfied: cut_cross_entropy in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth_zoo>=2025.3.14->unsloth) (25.1.1)\n",
      "Requirement already satisfied: pillow in d:\\ollama\\ollama_env\\lib\\site-packages (from unsloth_zoo>=2025.3.14->unsloth) (11.1.0)\n",
      "Requirement already satisfied: importlib-metadata in d:\\ollama\\ollama_env\\lib\\site-packages (from diffusers->unsloth) (8.5.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in d:\\ollama\\ollama_env\\lib\\site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in d:\\ollama\\ollama_env\\lib\\site-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from tyro->unsloth) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\ollama\\ollama_env\\lib\\site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\ollama\\ollama_env\\lib\\site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\ollama\\ollama_env\\lib\\site-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ollama\\ollama_env\\lib\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\ollama\\ollama_env\\lib\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ollama\\ollama_env\\lib\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\ollama\\ollama_env\\lib\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\ollama\\ollama_env\\lib\\site-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\ollama\\ollama_env\\lib\\site-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\ollama\\ollama_env\\lib\\site-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\ollama\\ollama_env\\lib\\site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\ollama\\ollama_env\\lib\\site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\ollama\\ollama_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ollama\\ollama_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: unsloth 2025.3.17\n",
      "Uninstalling unsloth-2025.3.17:\n",
      "  Successfully uninstalled unsloth-2025.3.17\n",
      "Collecting git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-qujn90bl\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 65b8975c5fb65e6c08726f228877ba6b6601f2ba\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml): started\n",
      "  Building wheel for unsloth (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for unsloth: filename=unsloth-2025.3.17-py3-none-any.whl size=197137 sha256=c040c2b1d8717babe6964d6ec8e148cf7446a5e1ac5d712636bc5a168c4cfcf9\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-hh7u4f3t\\wheels\\60\\3e\\1f\\e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2025.3.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-qujn90bl'\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "Using Unsloth, you load the DeepSeek-R1 Distilled Llama-8B model, a smaller, faster version of the Llama model optimised for performance while retaining accuracy.\n",
    "Along with the model, you also load its tokeniser. The tokeniser breaks down input text into smaller units (tokens) that the model can process.AttributeError\n",
    "\n",
    "## Importance?\n",
    "Loading the model and tokenizer is the foundation for fine-tuning since they define how text inputs are processed and predictions are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:35:22.644485Z",
     "iopub.status.busy": "2025-01-28T19:35:22.644139Z",
     "iopub.status.idle": "2025-01-28T19:36:19.089957Z",
     "shell.execute_reply": "2025-01-28T19:36:19.089148Z",
     "shell.execute_reply.started": "2025-01-28T19:35:22.644457Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10012\\2543187387.py:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp950' codec can't decode byte 0xcf in position 3539: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[32m      5\u001b[39m     model_name = \u001b[33m\"\u001b[39m\u001b[33munsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     max_seq_length = \u001b[32m2048\u001b[39m,\n\u001b[32m      7\u001b[39m     dtype = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m      8\u001b[39m     load_in_4bit = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ollama\\ollama_env\\Lib\\site-packages\\unsloth\\__init__.py:219\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msave\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ollama\\ollama_env\\Lib\\site-packages\\unsloth\\models\\__init__.py:15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[38;5;250m   \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLlamaModel\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, FastVisionModel, FastTextModel, FastModel\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmistral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastMistralModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ollama\\ollama_env\\Lib\\site-packages\\unsloth\\models\\llama.py:2743\u001b[39m\n\u001b[32m   2740\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2742\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PatchFastRL\n\u001b[32m-> \u001b[39m\u001b[32m2743\u001b[39m \u001b[43mPatchFastRL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFastLanguageModel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mFastLlamaModel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ollama\\ollama_env\\Lib\\site-packages\\unsloth\\models\\rl.py:742\u001b[39m, in \u001b[36mPatchFastRL\u001b[39m\u001b[34m(algorithm, FastLanguageModel)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mPatchFastRL\u001b[39m(algorithm = \u001b[38;5;28;01mNone\u001b[39;00m, FastLanguageModel = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m FastLanguageModel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: PatchRL(FastLanguageModel)\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m     \u001b[43mpatch_trl_rl_trainers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(algorithm) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m algorithm.islower():\n\u001b[32m    744\u001b[39m         PatchRLStatistics(algorithm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ollama\\ollama_env\\Lib\\site-packages\\unsloth\\models\\rl.py:735\u001b[39m, in \u001b[36mpatch_trl_rl_trainers\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    733\u001b[39m all_trainers = [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_trainers \u001b[38;5;28;01mif\u001b[39;00m x.islower() \u001b[38;5;129;01mand\u001b[39;00m x.endswith(\u001b[33m\"\u001b[39m\u001b[33m_trainer\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m trainer \u001b[38;5;129;01min\u001b[39;00m all_trainers:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[43m_patch_trl_rl_trainers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ollama\\ollama_env\\Lib\\site-packages\\unsloth\\models\\rl.py:555\u001b[39m, in \u001b[36m_patch_trl_rl_trainers\u001b[39m\u001b[34m(trainer_file)\u001b[39m\n\u001b[32m    552\u001b[39m RLTrainer_source = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn]\u001b[39m\u001b[33m{\u001b[39m\u001b[33m3,}\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, RLTrainer_source)\n\u001b[32m    554\u001b[39m \u001b[38;5;66;03m# Create new function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m created_module = \u001b[43mcreate_new_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUnsloth\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mRLTrainer_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mRLTrainer_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrl.trainer.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrainer_file\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Patch Trainer\u001b[39;00m\n\u001b[32m    564\u001b[39m exec(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrl.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRLTrainer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = created_module.Unsloth\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRLTrainer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ollama\\ollama_env\\Lib\\site-packages\\unsloth_zoo\\compiler.py:336\u001b[39m, in \u001b[36mcreate_new_function\u001b[39m\u001b[34m(name, new_source, model_location, functions, prepend, append, overwrite, add_torch_compile)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# Check if file was already created!\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite \u001b[38;5;129;01mand\u001b[39;00m os.path.isfile(function_location):\n\u001b[32m    334\u001b[39m \n\u001b[32m    335\u001b[39m     \u001b[38;5;66;03m# Check if exactly equivalent\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(function_location, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: file_source = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file_source != write_new_source:\n\u001b[32m    339\u001b[39m         overwrite = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'cp950' codec can't decode byte 0xcf in position 3539: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Parameter-Efficient Fine-Tuning (PEFT) \n",
    "The **FastLanguageModel.get_peft_model** function modifies a pre-trained language model to use PEFT techniques, which allow fine-tuning of the model using fewer resources and parameters.\n",
    "\n",
    "The method introduces additional tunable parameters (like LoRA matrices) to specific layers of the model while freezing most of the original model weights.\n",
    "\n",
    "***In this Project, we use LORA to fine-tune the DeepSeek R1 LLM.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it takes the existing model. Then, it applies a technique called PEFT (Parameter-Efficient Fine-Tuning).  Think of it as adding small, adjustable \"knobs\" instead of changing the whole engine.\n",
    "\n",
    "r=4 and lora_alpha=16 are just settings for how many \"knobs\" and how sensitive they are.  target_modules specifies where these knobs are attached – specifically, parts of the model that handle questions, keys, values, and outputs.  lora_dropout=0 means no \"knobs\" are randomly turned off during training.\n",
    "\n",
    "bias=\"none\" means no extra adjustments to the model's biases. use_gradient_checkpointing=\"unsloth\" is a memory-saving trick for training. random_state=42 ensures we get the same results if we run this again.  use_rslora=False and loftq_config=None are more advanced settings that are turned off here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"unsloth @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:37:14.224138Z",
     "iopub.status.busy": "2025-01-28T19:37:14.223695Z",
     "iopub.status.idle": "2025-01-28T19:37:20.051562Z",
     "shell.execute_reply": "2025-01-28T19:37:20.050877Z",
     "shell.execute_reply.started": "2025-01-28T19:37:14.224087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FastLanguageModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mFastLanguageModel\u001b[49m.get_peft_model(\n\u001b[32m      2\u001b[39m     model,\n\u001b[32m      3\u001b[39m     r = \u001b[32m4\u001b[39m,\n\u001b[32m      4\u001b[39m     target_modules = [\u001b[33m\"\u001b[39m\u001b[33mq_proj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mk_proj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mv_proj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mo_proj\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      5\u001b[39m     lora_alpha = \u001b[32m16\u001b[39m,\n\u001b[32m      6\u001b[39m     lora_dropout = \u001b[32m0\u001b[39m,\n\u001b[32m      7\u001b[39m     bias = \u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     use_gradient_checkpointing = \u001b[33m\"\u001b[39m\u001b[33munsloth\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     random_state = \u001b[32m42\u001b[39m,\n\u001b[32m     10\u001b[39m     use_rslora = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     11\u001b[39m     loftq_config = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'FastLanguageModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 4,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 42,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vicgalle/alpaca-gpt4 dataset is a collection of 52,000 instruction-following instances designed to fine-tune language models (LLMs). It was created by Vic Galie and is available on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:44:43.524760Z",
     "iopub.status.busy": "2025-01-28T19:44:43.524338Z",
     "iopub.status.idle": "2025-01-28T19:44:46.104782Z",
     "shell.execute_reply": "2025-01-28T19:44:46.103935Z",
     "shell.execute_reply.started": "2025-01-28T19:44:43.524698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files = \"./medquad.csv\",\n",
    "    split = \"train\",\n",
    ")\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files = \"./medquad.csv\",\n",
    "    split = \"train\",\n",
    ")\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:44:47.831145Z",
     "iopub.status.busy": "2025-01-28T19:44:47.830803Z",
     "iopub.status.idle": "2025-01-28T19:44:47.837464Z",
     "shell.execute_reply": "2025-01-28T19:44:47.836520Z",
     "shell.execute_reply.started": "2025-01-28T19:44:47.831121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda row: {\n",
    "    \"question\": row[\"question\"],\n",
    "    \"answer\": f\"{row['answer']}\\n\\nCategory: {row['focus_area']}\",\n",
    "    \"focus_area\": row[\"focus_area\"]  # Keeping it for reference, can be removed if not needed\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's format the dataset in a way suitable for conversational AI training using the ShareGPT format, which is designed for multi-turn conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:44:48.368943Z",
     "iopub.status.busy": "2025-01-28T19:44:48.368559Z",
     "iopub.status.idle": "2025-01-28T19:45:07.176530Z",
     "shell.execute_reply": "2025-01-28T19:45:07.175532Z",
     "shell.execute_reply.started": "2025-01-28T19:44:48.368914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import to_sharegpt\n",
    "\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt=\"{question}\",\n",
    "    output_column_name=\"answer\",\n",
    "    conversation_extension=0,  # Select more to handle longer conversations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:45:07.178493Z",
     "iopub.status.busy": "2025-01-28T19:45:07.178263Z",
     "iopub.status.idle": "2025-01-28T19:45:10.130543Z",
     "shell.execute_reply": "2025-01-28T19:45:10.129425Z",
     "shell.execute_reply.started": "2025-01-28T19:45:07.178473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:45:10.132983Z",
     "iopub.status.busy": "2025-01-28T19:45:10.132675Z",
     "iopub.status.idle": "2025-01-28T19:45:10.139400Z",
     "shell.execute_reply": "2025-01-28T19:45:10.138464Z",
     "shell.execute_reply.started": "2025-01-28T19:45:10.132960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset[0]['conversations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizable Chat Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes a dataset, formats it into a chat-friendly format, and then applies a template so that the AI can understand the instructions and responses correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:45:10.140900Z",
     "iopub.status.busy": "2025-01-28T19:45:10.140594Z",
     "iopub.status.idle": "2025-01-28T19:45:17.006478Z",
     "shell.execute_reply": "2025-01-28T19:45:17.005447Z",
     "shell.execute_reply.started": "2025-01-28T19:45:10.140876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chat_template = \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n",
    "\n",
    "### Instruction:\n",
    "{INPUT}\n",
    "\n",
    "### Response:\n",
    "{OUTPUT}\"\"\"\n",
    "\n",
    "from unsloth import apply_chat_template\n",
    "dataset = apply_chat_template(\n",
    "    dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    chat_template = chat_template,\n",
    "    # default_system_message = \"You are a helpful assistant\", << [OPTIONAL]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_data = dataset_split[\"train\"]\n",
    "val_data = dataset_split[\"test\"]\n",
    "\n",
    "print(train_data[0]['conversations'], '\\n', val_data[0]['conversations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define and configure a fine-tuning trainer for a language model using the **SFTTrainer** class from the **trl** library (likely for fine-tuning language models) and transformers' **TrainingArguments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses SFTTrainer from the trl library, which is specifically for Supervised Fine-Tuning.  This means the model will learn from the dataset of instructions and responses you prepared earlier.\n",
    "\n",
    "It takes the model (the tuned language model), the tokenizer (for breaking down text), and the dataset. dataset_text_field=\"text\" tells it where the actual text is in the dataset.  max_seq_length=2048 limits the length of text sequences the model processes at once. dataset_num_proc=2 uses two processes to prepare the data, and packing=False disables a specific data packing technique.\n",
    "\n",
    "Then, it configures the training process with TrainingArguments.  per_device_train_batch_size=2 means each training device (like a GPU) will process 2 examples at a time. gradient_accumulation_steps=4 combines the gradients from 4 batches to simulate a larger batch size. warmup_steps=5 gradually increases the learning rate at the beginning. max_steps=20 limits the total training steps. learning_rate=2e-4 sets the learning rate.  fp16 and bf16 control the precision of calculations (using either half-precision or bfloat16 if supported, for faster training). logging_steps=1 logs training progress every step. optim=\"adamw_8bit\" specifies the optimizer. weight_decay=0.01 is a regularization technique. lr_scheduler_type=\"linear\" sets how the learning rate changes over time. seed=3407 ensures reproducibility. output_dir=\"outputs\" specifies where to save the trained model. report_to=\"none\" disables reporting to services like WandB (Weights & Biases).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:45:17.008092Z",
     "iopub.status.busy": "2025-01-28T19:45:17.007726Z",
     "iopub.status.idle": "2025-01-28T19:46:03.412875Z",
     "shell.execute_reply": "2025-01-28T19:46:03.411741Z",
     "shell.execute_reply.started": "2025-01-28T19:45:17.008053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 20,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer orchestrates the fine-tuning process by combining the model, tokenizer, dataset, and hyperparameters. It ensures efficient training, taking advantage of mixed precision (fp16 or bf16) and memory-efficient optimizations (e.g., AdamW with 8-bit precision). Additionally, it handles sequence preprocessing, gradient accumulation, and learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:46:03.414275Z",
     "iopub.status.busy": "2025-01-28T19:46:03.413997Z",
     "iopub.status.idle": "2025-01-28T19:52:29.397069Z",
     "shell.execute_reply": "2025-01-28T19:52:29.396239Z",
     "shell.execute_reply.started": "2025-01-28T19:46:03.414249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLAMA is a powerful tool that enables you to run large language models (LLMs) directly on your own computer (laptop or desktop).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T20:22:33.733920Z",
     "iopub.status.busy": "2025-01-28T20:22:33.733430Z",
     "iopub.status.idle": "2025-01-28T20:23:25.235067Z",
     "shell.execute_reply": "2025-01-28T20:23:25.233937Z",
     "shell.execute_reply.started": "2025-01-28T20:22:33.733884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T20:23:25.236859Z",
     "iopub.status.busy": "2025-01-28T20:23:25.236588Z",
     "iopub.status.idle": "2025-01-28T20:29:56.857808Z",
     "shell.execute_reply": "2025-01-28T20:29:56.856335Z",
     "shell.execute_reply.started": "2025-01-28T20:23:25.236835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"model\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the OLLAMA Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-28T20:29:56.858594Z",
     "iopub.status.idle": "2025-01-28T20:29:56.859025Z",
     "shell.execute_reply": "2025-01-28T20:29:56.858824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "import time\n",
    "time.sleep(3)\n",
    "print(tokenizer._ollama_modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command registers the fine-tuned model (deepseek_finetuned_model) with Ollama. Once registered:\n",
    "\n",
    "- The model can be run locally using Ollama.\n",
    "- It becomes accessible for further tasks, such as querying, evaluating, or deploying in specific applications.\n",
    "- Ollama ensures the model is formatted and stored correctly for efficient usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ollama create deepseek_finetuned_model -f ./model/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=\"deepseek_finetuned_model\",\n",
    "            messages=[{ \"role\": \"user\", \"content\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\"\n",
    "            },\n",
    "                      ])\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, 1, 2, 3, 5, 8, 13, 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=\"deepseek_finetuned_model\",\n",
    "                       messages=[{\"role\": \"user\",\n",
    "                                  \"content\": \"How to add chart to a document?\"},\n",
    "                      ])\n",
    "\n",
    "Markdown(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a chart to a document, follow these steps:\n",
    "\n",
    "- Insert a Table: Start by inserting a table into the document. You can do this using the 'Table' tool in most word processors.\n",
    "- Insert Data: Add data into the table. Ensure that your data is properly formatted and organised before adding the chart.\n",
    "- Choose a Chart Type: Select the type of chart you want to create from the available options (e.g., bar chart, pie chart, line graph, etc.).\n",
    "- Edit the Chart Data: Add the necessary data points and formatting to the chart using the chart editor that appears once the chart is selected.\n",
    "- Format the Table and Chart Together: Make sure the table and chart work well together by adjusting alignment, spacing, and other design elements as needed.\n",
    "\n",
    "For more detailed instructions, you may want to consult a guide or use a tool such as Microsoft Word's chart features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
